| **Category**                              | **Concepts to Cover**                                                         | **Why Important (Interview Relevance)**                                                                                                                   |
| ----------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Text Representation**                   | TF-IDF, Word2Vec, GloVe, BERT embeddings, Sentence Transformers               | You’ll need to design pipelines that transform raw text into usable vector representations for downstream models (retrieval, ranking, anomaly detection). |
| **Semantic Search**                       | Dense Retrieval (e.g., DPR, ColBERT), Vector Databases (Faiss, ScaNN, Milvus) | For ML design questions around search or recommendations — e.g., “Design semantic search for marketplace items.”                                          |
| **Document Understanding**                | Transformers (BERT, RoBERTa, DistilBERT), Long Context Models                 | Understanding how to extract meaning from listings, chat messages, or descriptions at scale.                                                              |
| **Text Classification**                   | Spam detection, toxicity detection, intent classification                     | Directly relevant to fraud/anomaly questions — e.g., “Detect fraudulent product descriptions.”                                                            |
| **Text Similarity & Clustering**          | Cosine similarity, nearest neighbor search, hierarchical clustering           | Used in detecting duplicate listings, fake reviews, or related items.                                                                                     |
| **Sequence Models & Contextual Encoding** | RNNs, LSTMs, Transformers                                                     | Foundational understanding helps justify design trade-offs in latency or interpretability.                                                                |
| **Multilingual NLP**                      | Cross-lingual embeddings, multilingual transformers (mBERT, XLM-R)            | Mercari and many marketplaces operate globally — you may be asked about supporting multiple languages efficiently.                                        |
| **Summarization & Keyword Extraction**    | Extractive vs abstractive summarization, attention mechanisms                 | Useful in designing systems for content moderation or summarizing user-generated content.                                                                 |
| **LLM Integration**                       | Prompt engineering, vector databases, retrieval-augmented generation (RAG)    | Interviewers may explore how you’d use LLMs to improve recommendations or search results.                                                                 |
