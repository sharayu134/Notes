* Sigmoid
  * output is from 0 to 1  sigmoid(a) = - 1/(1+e^-a)
  * <img width="1445" height="766" alt="image" src="https://github.com/user-attachments/assets/e562c670-d18b-4a91-b1a3-410f4c7b91de" />
  * slower in training

* Relu -
  * Rectified Linear Unit - ReLU(a) = max(0,a)
  * <img width="1445" height="766" alt="image" src="https://github.com/user-attachments/assets/b54974fe-467a-4caf-8d04-45ad70077e6f" />
  * faster in training
  * <img width="1445" height="683" alt="image" src="https://github.com/user-attachments/assets/70c874dc-5965-436d-b3b5-cd2663d063b5" />

* Softmax
  * It gives the probability of the output values from 0 to 1
  * <img width="559" height="213" alt="image" src="https://github.com/user-attachments/assets/914e5c07-f548-441e-97ff-343780e88078" />
  * so with softmax is mostly used at end when we are doing multiclass classification like recognising handwritten digits  In output we wanna know what number from 0 to 9 it is



