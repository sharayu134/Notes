{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Always try to simplify the data given in neural network !!!"
      ],
      "metadata": {
        "id": "7I9ekQpX0ve9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the input is going to be images pf cats and dogs but the images are going to be in various size and shapes so first thing we do is resize them to uniform size so that we can feed to the CNN, so you have many options with m=images you can rezise,pad, crop, flip, rotate etc"
      ],
      "metadata": {
        "id": "sWf0DlEuxyaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting an input image to grayscale before feeding it into a Convolutional Neural Network (CNN) is a common preprocessing step done for two main reasons: to **reduce complexity** and to **focus on essential features** when color isn't important.\n",
        "\n",
        "---\n",
        "### Reducing Computational Complexity ⚙️\n",
        "The biggest advantage of using grayscale images is that it makes the CNN model simpler and faster to train.\n",
        "\n",
        "* **Fewer Parameters:** A color image has three channels (Red, Green, Blue - RGB), while a grayscale image has only one channel representing intensity (shades of gray).\n",
        "* **Faster Training:** With only one channel to process instead of three, the number of calculations the network has to perform at each layer is significantly reduced. This means the model will train faster and require less memory and computational power.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "### Focusing on Relevant Features\n",
        "For many computer vision tasks, the color of an object is irrelevant and can even be considered \"noise\" that distracts the model. By converting to grayscale, you force the CNN to learn from more fundamental features like **shape, texture, and edges**.\n",
        "\n",
        "* **Good Use Cases:**\n",
        "    * **Character Recognition (OCR):** The shape of a letter is what matters, not its color.\n",
        "    * **Medical Imaging:** In many X-rays or MRI scans, the important information is in the contrast and structure, not color.\n",
        "    * **Some Object Detection:** For identifying the shape of a car or a pedestrian, the outline and texture are often more critical than the specific color.\n",
        "\n",
        "---\n",
        "### When Not to Use Grayscale\n",
        "It's important to remember that this technique is not suitable for every problem. You should **avoid converting to grayscale** when color is a critical feature for classification.\n",
        "\n",
        "* **Bad Use Cases:**\n",
        "    * **Fruit Ripeness Detection:** Distinguishing between a green (unripe) and a red (ripe) apple requires color information.\n",
        "    * **Traffic Light Recognition:** The model must be able to differentiate between red, yellow, and green lights.\n",
        "    * **Flower Species Classification:** The color of the petals is often a key identifier for different types of flowers.\n",
        "\n",
        "In summary, the decision to use grayscale is a strategic choice. If color doesn't add valuable information for your specific problem, converting to grayscale is an effective way to simplify your model, speed up training, and potentially reduce overfitting."
      ],
      "metadata": {
        "id": "LOAJ0QFO0Fj6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i92Y_QC4o5uJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "REBUILD_DATA = False #Keep True only when make_training_data() to be called\n",
        "\n",
        "# data source https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
        "\n",
        "class DogsVSCats():\n",
        "  IMG_SIZE = 50\n",
        "  CATS = \"drive/MyDrive/kagglecatsanddogs_5340/PetImages/Cat\"\n",
        "  DOGS = \"drive/MyDrive/kagglecatsanddogs_5340/PetImages/Dog\"\n",
        "  LABELS = {CATS: 0, DOGS: 1}\n",
        "  training_data = []\n",
        "  catcount = 0 #here we have count to check if have class imbalance\n",
        "  dogcount = 0\n",
        "\n",
        "  def make_training_data(self):\n",
        "    for label in self.LABELS:\n",
        "      print(label)\n",
        "      for f in tqdm(os.listdir(label)):\n",
        "        if \"jpg\" in f:\n",
        "          try:\n",
        "            img = cv2.imread(os.path.join(label,f), cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:  # Check if image loaded successfully\n",
        "              img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
        "              self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]]) #n.eye gives array where there are 1s in diagonal and everythings else is 0, you can use it for creating one hot vectors\n",
        "\n",
        "              if label == self.CATS:\n",
        "                self.catcount += 1\n",
        "              elif label == self.DOGS:\n",
        "                self.dogcount += 1\n",
        "          except Exception as e: #some of images are not good so\n",
        "            print(f\"Error processing {f}: {e}\")\n",
        "            pass\n",
        "\n",
        "    # Move shuffle and save outside the loop - should only happen once after all data is collected\n",
        "    np.random.shuffle(self.training_data)\n",
        "\n",
        "    # Convert to numpy arrays with proper structure\n",
        "    X = np.array([item[0] for item in self.training_data])\n",
        "    y = np.array([item[1] for item in self.training_data])\n",
        "\n",
        "    # Save as separate arrays or as a dictionary\n",
        "    # np.save(\"training_idata.npy\", self.training_data, allow_pickle=True)\n",
        "    np.save(\"training_images.npy\", X)\n",
        "    np.save(\"training_labels.npy\", y)\n",
        "\n",
        "    print(\"Cats:\", self.catcount)\n",
        "    print(\"Dogs:\", self.dogcount)\n",
        "    print(f\"Total training samples: {len(self.training_data)}\")\n",
        "    print(f\"Image shape: {X.shape}\")\n",
        "    print(f\"Labels shape: {y.shape}\")\n",
        "\n",
        "if REBUILD_DATA:\n",
        "  dogsvcats = DogsVSCats()\n",
        "  dogsvcats.make_training_data()\n",
        "\n",
        "# Load the training data\n",
        "# X = np.load(\"training_images.npy\")\n",
        "# y = np.load(\"training_labels.npy\")\n",
        "# print(f\"Loaded {len(X)} training samples\")\n",
        "# print(f\"Image shape: {X.shape}\")\n",
        "# print(f\"Labels shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab034fd0"
      },
      "source": [
        "training_data_i = np.load(\"training_images.npy\", allow_pickle=True)\n",
        "training_data_l = np.load(\"training_labels.npy\", allow_pickle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(training_data_i[0], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "vJUY0b024Esn",
        "outputId": "f90c3217-a22e-43d2-cc26-3c5c69176c81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANWJJREFUeJzt3X9s1dd5x/HHxNgGbGyMwcYYE1JYyE9IIBCTrWkIDYvSlCxIy6RKy7poVTJgCWTagrYmW7UJ1khNmpbQqsuINi31RFcSperSZiRxtBYImFAgBUJaAgZjGwz+gQGbwHd/RFhxuefz2PfinBt4vyRLjR+fc7/33O/104uf55ycJEkSAwDgUzYk9gUAAC5PJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUuYM18apVq+zpp5+2pqYmmzZtmn3nO9+xWbNmuePOnTtnjY2NVlRUZDk5OYN1eQCAQZIkiXV2dlplZaUNGSI+5ySDoLa2NsnLy0v+7d/+LXnvvfeSv/iLv0hKSkqS5uZmd2xDQ0NiZnzxxRdffH3GvxoaGuTv+5wkufibkc6ePdtuueUW++53v2tmH3+qmTBhgi1ZssSeeOIJOba9vd1KSkps1qxZlpt74Qe0TD4VeWMH6xOX/H8AjnSvyRunXvbBWodMrsmjxl5xxRVpP+YgvD1cg/mYau5z584N2uMqg/W4Z8+eHZR5M3nvZPJcB2udMrnfQtd09uxZ2759u7W1tVlxcXFw/EX/J7ienh6rr6+35cuX935vyJAhNm/ePNuwYcMFP9/d3W3d3d29/93Z2fnxheXmkoCMBNRfJKDM577UEtBg3ceXUwLy7sVMf6de9CKEo0eP2tmzZ628vLzP98vLy62pqemCn1+xYoUVFxf3fk2YMOFiXxIAIAtFr4Jbvny5tbe39341NDTEviQAwKfgov8TXFlZmV1xxRXW3Nzc5/vNzc1WUVFxwc/n5+dbfn7+xb4MAECWu+gJKC8vz2bMmGHr16+3++67z8w+/rfL9evX2+LFi/s9T05OTsp/PxzMvwEpmfwdZ7DmjfW3GhXP5O8XmayFetzTp08HY3l5eWk/Zjb+nc2jrmmw7nHvbxfqcdVY9+8LGTyfTB5XyWSs+lumKriI8XfM/hqUPqBly5bZgw8+aDNnzrRZs2bZs88+a11dXfbVr351MB4OAPAZNCgJ6IEHHrAjR47Yk08+aU1NTTZ9+nR77bXXLihMAABcvgZtJ4TFixcP6J/cAACXl+hVcACAyxMJCAAQBQkIABAFCQgAEMWgFSFkasiQIRe9LyFWz81gjY01b7r9LR999JGcV/U5eL0kJ0+eDMbU697a2irnVX1CXg+RiqtrymQ/Ma/nQz3uYO3FN1j9RZ5M7qd0e5M8mbxn0319BvMxQ+vU32vlExAAIAoSEAAgChIQACAKEhAAIAoSEAAgChIQACCKrC3DDvFKOgdrG3XFu6ZMtuVPt1w606N00x2rtoXPZJ3UkQpmH5/EG3L8+PFgTF2vmdnQoUODsUxKngsLC4Mx71RgdU2ZrLGSSWl+JseexyqHTvd9591PmTwf9biZHDs/GPr72vAJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQRdb2AYWOY/Dq91U9fCa1/5nIZOt9JZOxql+hp6dHjj1z5kwwpnpU1Dgzs66urmDswIEDcmx7e7uMh2SyhiNGjJBx1QvR0dERjB08eFDOe+WVVwZj1dXVcqwyWPdiJv1Sag1j9QSq56N+/3hjs/H5DDY+AQEAoiABAQCiIAEBAKIgAQEAoiABAQCiIAEBAKLI2jLsrq4uy8298PK8UsW8vLxgLNV8/Z073e3ZzTIrc0y3vNLbFl6VAXulpB999FEwtn///mDs8OHDcl71uN7zGaxSUrX+qmzcTN9v6l7zytXff//9YKyiokKOLSgoCMYyWcNMyovTLWv2tvwfrKMc1LyDtYbe4w5WiXYmpeH9mj+j0QAApIkEBACIggQEAIiCBAQAiIIEBACIggQEAIgia8uwjx49mrIEcNy4ce64kOHDh8uxx44dC8YqKyuDMW9XZFUG6ZU5KqoE0iuPVNfU0tIixzY1NQVj3d3dwZhXBq/KUFX5sJlZSUlJMKZe91OnTsl51fPp7OyUY9Wu4mrXcG+dVEm6t5P2VVddFYxlUvIcYzfsTAxWeXEmLRmD2c6hZPLaZYpPQACAKEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgChIQACAKLK2D2jUqFEp+xLUcQtmZlVVVcHYyZMn5VjVS6LGqv4hM7Py8vJgbNiwYXKsqsNX/SDHjx+X8x46dCgY856PetxMekny8/ODsenTp8uxY8aMCcbUEQXePXHkyJFgbN++fXLshx9+GIypIy28nid1T6h72Ewf9TBY/WpeL0kmvTHpyqQ3ScnkiIhMem4Gq5fKW//Q+72/z4VPQACAKEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgCiytgy7qakpZWlhUVGRHKfKWz3FxcXBmCqNbW5ulvPu378/GMvkiAK1pb8qszYzO3HiRDDmlW2mWy7qjVPHWqjnaqbL79XrWlZWJudVJdwqZmZWWloajO3duzcY80rD1b347rvvyrETJkwIxiZOnBiMeeuvqHvNTL8HVFm/R5Ume89HtRqk+5hm+r2VScl5jKMaLsbj8gkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABBF1vYBDR06NGVNfUtLixynej48qkY/k/4i1Rvgbb2vjkZQ/SJq2/1Mpbv1u/dcx44dG4zNnDlTjlX9Io2NjcHYyJEj5byql8c7GkT1t6g13Llzp5y3vb09rXnN9Fq0trYGY6NHj5bzqh4u78iRo0ePBmPqiI7u7m45r3oPeP0rlZWVwZh6Xb2jTNQ9U1hYKMeq56PWyes1zOS4htDY/vY08QkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQxYDLsN9++217+umnrb6+3g4fPmzr1q2z++67rzeeJIk99dRT9oMf/MDa2trstttus9WrV9uUKVMG9Dhjx45NWT7olU+q8j+v5FbNrbbP90pUVXmlKn0106XLqnzVk24ptZneyl6toVe2fPXVVwdjXinvqFGjgrHq6upg7NSpU3LeTKjS/WuuuSYY89b/nXfeCca8YwTU3Oq1O3LkiJxXlQh7ZcAq3tXVFYypEnkzXRLttVWoI1RUyblHHU1x/PhxOVa1mKjfI5mUYXv3U+j3bX9Luwf8Cairq8umTZtmq1atShn/5je/ac8995x973vfs02bNtmIESNs/vz5dvr06YE+FADgEjbgT0B333233X333SljSZLYs88+a3//939vCxYsMDOzf//3f7fy8nJ7+eWX7U/+5E8yu1oAwCXjov4NaN++fdbU1GTz5s3r/V5xcbHNnj3bNmzYkHJMd3e3dXR09PkCAFz6LmoCampqMjOz8vLyPt8vLy/vjf2uFStWWHFxce+XOjIYAHDpiF4Ft3z5cmtvb+/9amhoiH1JAIBPwUVNQBUVFWZm1tzc3Of7zc3NvbHflZ+fbyNHjuzzBQC49F3U3bAnTZpkFRUVtn79eps+fbqZmXV0dNimTZvskUceGdiF5eamLB8sKSmR41TZptox1kyXNasyYLUrtZlffqyosnJVou3tFKzKUNXO0ma6hFs9V1Uq7Y31Sl9V2aea11unX/ziF8GYV9mp7pmioqJgbNKkSXLeffv2BWPebvGf//zng7G6urpgzLvH1XP1St3VfaxKiL3S/KqqqmDMe+1UXMW89456P3stJupeVfe4Kv020+0p3u7eofddf8uwB5yATpw4YR988EHvf+/bt8+2bdtmpaWlVl1dbY899pj90z/9k02ZMsUmTZpkX//6162ysrJPrxAAAANOQFu2bLE77rij97+XLVtmZmYPPvigvfjii/Y3f/M31tXVZV/72tesra3Nfv/3f99ee+019ywYAMDlZcAJ6Atf+IL8qJiTk2Pf+MY37Bvf+EZGFwYAuLRFr4IDAFyeSEAAgChIQACAKEhAAIAoLmof0MU0dOjQlD0A3vbgqtfnwIEDcqyqXe/p6QnGvJ4CVWefSe2/2jevv3X4F/uaVN/GuHHj5LyzZ88Oxrz+CvW4H374YTAW2iLqvDlz5gRjXm+M6qFQfU3eER3ne+xS2bx5sxyr7sX58+cHY2+88Yacd/jw4cGY17unjhlQPUTeMQPq2BB1fIRHPa7X86f6FFV/nZm+39QaeuvkHU2hhH5XeL9DzuMTEAAgChIQACAKEhAAIAoSEAAgChIQACAKEhAAIIqsLcMuKipKWUbpbe2uSl+9rfdVifdgnVPklV6qcsb+ljqm87iKKuu88sorg7GbbrpJzquOw/DWX5V4qxJhdb1mH2++G7J37145dubMmTIeMnr0aBnv7OwMxsrKyuRYtcZTpkwJxrx7Td1PqvTYTD9fVUqdyTV5Y1U5tbr/vXlLS0uDMXWfmunno9pPvJJz1UbilZWHft/2tw2ET0AAgChIQACAKEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgCiytg/o2LFjKevtve3zVa18Tk6OHKu2/FdblqteBY+3VXq6vDp8FfeOPpg3b14w1tDQEIzt2LFDznvttdcGY14vg+r/Uj0SHvXaLliwQI5VvUlq/dUxG2Yf98iFVFVVybE33HBDMKbeW+o4EjP9vvPucTW36t3z3s8FBQXBmNcHp+Lqd4G3TmotvPeder7qqAbv95PqA/KO0gg5e/asNTY2uj/HJyAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUWVuG3dXVlbIsUZVAmmVWDqpMnTo1GPO2YFdljl4paXt7ezCmSnmPHz8u51Xlot6RF7t27QrG5s6dG4yp4y7MzFpbW2VcaWtrC8auueaaYEytr5l+3dXW+mb6flOvu3dPjBkzJhj7zW9+I8eqYy1UKbtXtqyOefDed+o9rd47hYWFcl5vHRV1zep6vZL/EydOBGPecQxqLdTvgvLycjlvJke+hFoGOI4BAJDVSEAAgChIQACAKEhAAIAoSEAAgChIQACAKLK2DHv//v0pyygnTpwox6kSSa+UVO0UrHab9XbAVbsXd3V1ybFqF2i12+/o0aPlvC0tLcGYV3qpxqpSau+a1O65V111lRyrqNdHlTSb+btwK52dncGY2qHYu08VVWZtZvbOO+8EY2+88Ubaj6t2rVbvHTOzI0eOBGNqLbzdoxVV0mymX3f1/vB+F6gybfV+7s/cIV5JtGqPUK+rWXgtvN8h5/EJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQRdb2AeXm5qbsAcjLy5Pj1Bbs3vbtqq9Abb2v+hjMdM/BiBEj5Njm5uZgLJOjJ9Tz8forVH9LQ0NDMOb1Btx7773BmNff8sEHHwRjqpdh/Pjxcl51zIDqh/LmVq+d2rLf493jitq2/9ChQ3LssGHDgrGSkhI5Vt2rqjfG+12g5h03bpwcq6j72DsCQr23vD4f1TuW7mOamVVUVARj6v5X18RxDACArEYCAgBEQQICAERBAgIAREECAgBEQQICAESRtWXYV1xxRVrb0quST29r/VOnTgVjqgTSK3lWR0SordDN9PbtqrxbPaaZLv/2ylvVNR84cCAYU8ddeI4dOybjEyZMCMYOHz4cjB0/flzOe/DgwWBMla+a6ddHraF336tS3+LiYjlWlaTPnTs3GPvpT38q5x0+fHgw5pXkqlJrtYZeybka671n1XtAvT7ea6fWIpPjJVQJt3cchrpmb41Dv2e832u9j92vnwIA4CIjAQEAoiABAQCiIAEBAKIgAQEAoiABAQCiIAEBAKLI2j6gnJyclP0OqrbfTPdmdHV1pX096nG9vhnVe+H1Dai5VS+Pt6W/6oPweiRUT1R7e3sw1tHRIedtbW0Nxrwt/SsrK4Ox3bt3B2Oqz8rM7Nprrw3GMukrU2O9Hi7VL+Jt2a9e27FjxwZj1dXVcl7Vp+W9P9Rr29TUFIx5z9U7/kPxjlUI6e8xBKl47zs1t3esi6J+t6ljNszCv7+8e7h3fL9+CgCAi4wEBACIggQEAIiCBAQAiIIEBACIggQEAIhiQGXYK1assB//+Me2e/duGzZsmM2ZM8f+5V/+xa6++urenzl9+rQ9/vjjVltba93d3TZ//nx7/vnnrby8fGAXlpubssRPbftupssGvdJKVdapygq9ck9Vau2NVaW86vl45d3qcb2xaot2dRzA9OnT5byKKmU3Mzt06FAwpkqt29ra5LyqXNorL1alserYiokTJ8p51Vb3XvmrOhJj9uzZwdg999wj5126dGkw5r1n1Rqre80reVb3uFd+n8nrrmRylIP6/aR+T3jXq8q/vXaO0DX39yidAX0Cqqurs0WLFtnGjRvt9ddftzNnzthdd93Vp79m6dKl9uqrr9ratWutrq7OGhsb7f777x/IwwAALgMD+gT02muv9fnvF1980caOHWv19fX2+c9/3trb2+2FF16wl156qfdwqzVr1tg111xjGzdutFtvvfXiXTkA4DMto78Bne96Ly0tNTOz+vp6O3PmjM2bN6/3Z6ZOnWrV1dW2YcOGlHN0d3dbR0dHny8AwKUv7QR07tw5e+yxx+y2226z66+/3sw+3jYjLy/vgq01ysvLg1tqrFixwoqLi3u/1NHKAIBLR9oJaNGiRbZz506rra3N6AKWL19u7e3tvV8NDQ0ZzQcA+GxIazPSxYsX209+8hN7++23raqqqvf7FRUV1tPTY21tbX0+BTU3Nwc3Cc3Pz3crUgAAl54BJaAkSWzJkiW2bt06e+utt2zSpEl94jNmzLChQ4fa+vXrbeHChWZmtmfPHjtw4IDV1NQM6MKSJElZRpnJ7tFeGbbaLVuNVTtAm+mST1VmaqbLalXM26lZlcaq3ZbNdHnr5MmTg7E5c+bIedXjeq97fX19MKZ2avbKltXu6h51X6jdu71dkVVJ+n333SfHFhQUBGOqrFmN83g7Knv3akhnZ6eMq/dWJve4KoP37lM1drB4rR7qPeC97qHn663veQNKQIsWLbKXXnrJXnnlFSsqKur9u05xcbENGzbMiouL7aGHHrJly5ZZaWmpjRw50pYsWWI1NTVUwAEA+hhQAlq9erWZmX3hC1/o8/01a9bYn/3Zn5mZ2TPPPGNDhgyxhQsX9mlEBQDgkwb8T3CegoICW7Vqla1atSrtiwIAXPrYCw4AEAUJCAAQBQkIABAFCQgAEEVajaifhvz8/JQ15t7W4qrno6enR45V252rWFFRkZxX9QGdPHlSjlVzq7HeFviqoEQdI2Bmdvjw4WBs+/btwZjX86R6KLy+AlXmf9NNNwVj6hgHM+vTaP27vP4V9bpn8lxVH5DXQ6R6fdT1tra2ynlVv4jqr/OuST0fdfSHN693lIPqjVHr5PWVqd8j3jWpeybdNTTTPY7e8/Hm9vAJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEEXWlmGPHj06ZTnq7562mmpciFc2q7beV6WkXhm22oLdKytXj6vKcdva2uS8p0+fDsa8LdhVOfvBgweDscbGRjlvdXV1MOaVe6ry1vNHxqcyZsyYtOf1SlTV/aZKeb12AVVi75XyqtdWXa8qvTcz+9KXvhSM/fjHP5Zj1b34uc99LhjzjjZQce93gVpH9bp7R76oa1L3Wn/mDvGeqyrv9o6XCMX7e618AgIAREECAgBEQQICAERBAgIAREECAgBEQQICAERBAgIARJG1fUBlZWUp6+K9uvRMtm8/ceJEMJZJ34baXt/r2+js7JTxENXTZKafj9fzUVZWFowdOXIkGPurv/orOe9Pf/rTYEz1ipjp+8I7XkJR/QzesQnDhg1L6zHVa+M9rtcbo/rDTp06FYypnjMzfc2qN8/Mv+YQr9dEve5en5x6PupxM1mnTKjjVbx1Uu8dNe/FwCcgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFFlbhl1VVZXyqIKWlhY5rry8PBjzSiBVefGxY8eCMe9IBVXKqMqWzcwKCwuDMVVyrkpqzcza29uDsXHjxsmx6nFVKfuBAwfkvK2trcGYV36vSp5Vma9XKq3K5L2x6r5QW/p7JefqOIZMyovV63r06FE5byZlwOr1UWO9edVrl8kRKuqIFK+kXF2zdxyDoh7XO8okE6F7nOMYAABZjQQEAIiCBAQAiIIEBACIggQEAIiCBAQAiCJry7BPnz6dsowy3R2GzXT5qpkuYR05cmQw1tjYKOdVJcRjx46VY9WOvidPngzGvF151Y7Xal4vfvXVVwdj3s7fDQ0NwdiVV14px6rXtqCgIBh7+umn5bw33XRTMPblL39Zjk23vFhdr5nZAw88EIz93d/9nRx7ww03BGOqDH7evHly3tdeey0Y8153tbu3Klc/c+aMnFfdE941qblVubS3e7R6X3q/n7z3ZYhaQ7PMSt1Dz7e/u2jzCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEEXW9gGdPXs2ZR/FwYMH5bgPPvggGPOOGVC9Jm+//XYwNnv2bDnvzp07gzFvC3Z1vMSYMWOCMdW3ZKb7qby+JnVN6riMqqoqOa9ai9LSUjnW650J+eu//msZV0cjqL4ZM30khuoN8/o9/uM//iMYKy4ulmPVUQKZrP/nPve5YOyXv/ylHKt6RlQ/jtcTqMZ6RxSonhzVy+NdU2dnZzDmHeWgrrmnpyetcWa6Jyrdoxw4jgEAkNVIQACAKEhAAIAoSEAAgChIQACAKEhAAIAosrYMe8+ePSm3aVelx2a6lHT//v1yrCphveuuu4Kxd955R847c+bMYMwrUVUlkrt27QrGysrK5LyqhHjixIlyrCqxVMdL7N69W86rSpO9clB1bIVaQ+9+Uo/b1NQkx6rjPfLy8oIxr5RXPVevlFeVPBcVFQVjqnzbzOyqq64Kxv74j/9Yjn3llVeCMVXK7h2poF4777gSVdav7lMV83jHS6hjK9R70ivrV6+7J3TUg3cExHl8AgIAREECAgBEQQICAERBAgIAREECAgBEQQICAERBAgIARJG1fUBjx45N2dPj9V4okyZNkvHjx48HYz//+c+DsTvuuEPOq3oDbrjhBjlW9To0NDQEY14/iNoW3jvy4vbbbw/GVP+E2rLfzO9XUFTPh+qbOXHihJxX9eR4PUTq+ajXbs6cOXJe1WPhrWFJSUkwpu4JrzdJvXeqq6vl2Ouvvz4YU/1qe/fulfOq/hbvGBTV96TuJ9X7ZWZWWFgYjHn3YrpHI+Tn58u4+v3k9VqF4t643sfu108BAHCRkYAAAFGQgAAAUZCAAABRkIAAAFGQgAAAUQyorm/16tW2evVq+/DDD83M7LrrrrMnn3zS7r77bjMzO336tD3++ONWW1tr3d3dNn/+fHv++eetvLx8wBd2/PjxlGWHqszXTJdIHj16VI4tLS0NxtTj7tixQ86rSl+HDx8ux44fPz4YmzJlSjCmjmowM9uwYUMw5pUXq23hVfm3t1V9R0dHMOYdB6DKhFX5qlf6qo4vUCW1ZvpoCvW4Xgmr2nrf2wZfrbE6IkKVWZvp9fdaJ9TvB/W46j4002usfk+Y6dJlVa6ujnQx08dLeKXh6prUvF75trqf+ltOna4BfQKqqqqylStXWn19vW3ZssXmzp1rCxYssPfee8/MzJYuXWqvvvqqrV271urq6qyxsdHuv//+QblwAMBn24A+Ad177719/vuf//mfbfXq1bZx40arqqqyF154wV566SWbO3eumZmtWbPGrrnmGtu4caPdeuutF++qAQCfeWn/Dejs2bNWW1trXV1dVlNTY/X19XbmzBmbN29e789MnTrVqqur5T/3dHd3W0dHR58vAMClb8AJaMeOHVZYWGj5+fn28MMP27p16+zaa6+1pqYmy8vLu+DvHeXl5fLfgFesWGHFxcW9XxMmTBjwkwAAfPYMOAFdffXVtm3bNtu0aZM98sgj9uCDD9qvf/3rtC9g+fLl1t7e3vul9sgCAFw6Bry7XV5enk2ePNnMzGbMmGGbN2+2b3/72/bAAw9YT0+PtbW19fkU1NzcbBUVFcH58vPz3c3yAACXnox3wz537px1d3fbjBkzbOjQobZ+/XpbuHChmZnt2bPHDhw4YDU1NQOet6ysLGVZole2fObMmWCsqqpKjj106FAwNmrUqGDs2LFjcl5Vhj19+nQ59kc/+lEwpioMvb+lqWvyyjbVNd10003BmLej8r59+4KxWbNmybHd3d3BmCrh9nYNV2XA3hqr0tiRI0cGY94uz+mWnHuP297eHox5O6Sfr4RNZfTo0XKseu1UabLaRdvMbOvWrcGY97qrlgF1TV5Zv/o/22odvLnVzt/ePdHT0yPjg2lACWj58uV29913W3V1tXV2dtpLL71kb731lv3sZz+z4uJie+ihh2zZsmVWWlpqI0eOtCVLllhNTQ0VcACACwwoAbW0tNif/umf2uHDh624uNhuvPFG+9nPfmZf/OIXzczsmWeesSFDhtjChQv7NKICAPC7BpSAXnjhBRkvKCiwVatW2apVqzK6KADApY+94AAAUZCAAABRkIAAAFGQgAAAUWTcBzRYhgwZkrIW//Dhw3Kc6g3wGl7VcQ2ql2TBggVyXrVtv7fN/S233BKM/e///m8w5m2B39raGoxVVlbKserYCtUT5R3zoHq8VI+Kme5vUf0Vqh/KG9vW1ibHqq3sVQ+R16Oi1sm7x9V9rHrotm/fLudVRyrU1tbKsTfffHMwpvpbWlpa5LyK975TfX/q2Aqv50b1hnlHaaj+I3XPeEeZqJ4n78iL0D2ujjHp89j9+ikAAC4yEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgiqwtw66srExZ7qhKEc10KalXonrttdfK6wnp7OyU86oSyYKCAjlWbcGutlEvLi6W86oySVWObmZ23XXXBWM7d+4MxlSprtnHhx2GeMdwqEMRr7rqqmBM3S9m+vXxjpdQ5a2qrNwr5T158mRaMTNdGv72228HY6qVwJv3S1/6khyrjuFQJf+ZHF7plQmrdVRHjvzqV7+S86p1PH36tByr1lg9H+/3Xk5OTjDmlYaH7nE1Z5/x/fopAAAuMhIQACAKEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgiqztAyosLExZv66OETAz++IXvxiMqeMLzPSW/r/97W+DMdWrYKbr99W272Zm+/fvD8ZUH4TXc6P6i7y+ATVWyeQ4DNVTY2Y2Y8aMtOb1tvRvbm4Oxrw19nrWQrx7/NChQ2nNa6bXcezYscHYzJkz5byqb+add96RY1U/lep1U+8rM93D5d1Pqk9rx44dwdiIESPkvMro0aPTviZ1bIJ3pILqIfJ60ryjQzx8AgIAREECAgBEQQICAERBAgIAREECAgBEQQICAESRtWXYra2tKY9juOGGG+S49957Lxi78sor5di9e/cGY2PGjAnGvPLJtWvXBmO33nqrHKuOPlDXtG3bNjmvOq6hq6tLjlVlwqles/48ppkuOfeOPlDlr2qsV46r1t+jSlTVNvfjx4+X8954441pX5MqTVZb6Hslz+qYgT/8wz+UY7ds2ZJWzFNRURGMnTp1So4tKioKxtT7w2tRUK+7d0SEire1tcmxinqu3jWFjqLxjnE4j09AAIAoSEAAgChIQACAKEhAAIAoSEAAgChIQACAKLK2DPvQoUMpd2JVOxubmU2cODEY80oDVYm32tn4V7/6lZx3+PDhwZjaWdcsXOZopssnb7vtNjmvKnn2yoB/8YtfBGOnT58OxhobG+W86vXxykzLysqCMbX+XpmpuiZvt2tV1qx2KB7MHYhVmbwqSe/u7pbzqp3Oz5w5I8fW1NQEY1dddVUw9uqrr8p5m5qagjGvJWDXrl3BmCr59+4n9f7w7ic1tyqTV7tom+kdx73fmaGTALzX/Dw+AQEAoiABAQCiIAEBAKIgAQEAoiABAQCiIAEBAKIgAQEAosjaPqA5c+ak3Dq+oaFBjlNb73v9Fapv4IMPPgjGvOMLVI+Ed8yAOl4iVINv5h89UV1dHYwdOHBAjr3jjjuCsSNHjgRjqn/CTPeaqP4JM7Pjx48HY6rPYeTIkXJedc9495Pqq1F9Eqp/yEz3i3hj0z0OwDu2YrCoXrc/+IM/kGO9I0mUTPrZFHXPeH1Aqv9L9QGpNTTT94TqGzMz6+joGPCcn8QnIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBRZW4a9e/fulCWA3nbnqmzQK+VVpcmqXDpUinieKp/0jhlQJZSq5NYrx1Xl7GPGjJFj33333WDsuuuuC8a+/OUvy3lVSfTBgwflWHVfqKMnxo4dK+ctKSlJ6zHN9PNRJbcnT56U86oS7lStC5+kyqlVKa86PsJMvwdOnTolx6rnq9bJK4OfMmVKMKbuYTN9hIF63b33nXrtvDVWj6tiqg3Bo+4Js/Dv2/6W7fMJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQRdb2AZ08eTJlzby3Zbnqg/DGHj58OBjbsWNHMDZq1Cg5r1ffr6geopkzZwZj48aNk/OqYx68vgH1fGbNmhWMeX1Y6riGsrIyOVb1Haiep9/+9rdy3unTpwdjqqfDzKyzs1PGQ7yt7NU97vVfqPtJ9dVk0ofV2toqx44fPz4YU30oXh+Q8uabb8q46idMdw29sd79pMaq9fd6eTLpJwxdk7rWT+ITEAAgChIQACAKEhAAIAoSEAAgChIQACAKEhAAIIqMyrBXrlxpy5cvt0cffdSeffZZM/u41Pbxxx+32tpa6+7utvnz59vzzz9v5eXlA5q7urra8vPzL/i+V1KoSlSbmprkWBVX2/J75baTJ08Oxo4ePSrHXn/99cGYKtvcunWrnFeVnHvb5xcXFwdjP/zhD4Mxr1xdlbB661RaWprWvF6ZqToqYOrUqXKsui9UqbsqATbTpdbe81Fr0dLSEoxVVFSkfU3esRW7d+8OxiZMmBCMee0N6v3h/R5RLQPquXpHaajX1mt/UK+dGuvdE17ZvxJ6DbzHPC/tT0CbN2+273//+3bjjTf2+f7SpUvt1VdftbVr11pdXZ01Njba/fffn+7DAAAuUWkloBMnTthXvvIV+8EPftDn/9W2t7fbCy+8YN/61rds7ty5NmPGDFuzZo398pe/tI0bN160iwYAfPallYAWLVpk99xzj82bN6/P9+vr6+3MmTN9vj916lSrrq62DRs2pJyru7vbOjo6+nwBAC59A/4bUG1trW3dutU2b958Qaypqcny8vIu+HtJeXl58O8rK1assH/8x38c6GUAAD7jBvQJqKGhwR599FH7z//8T/fs+f5avny5tbe3936pfbsAAJeOASWg+vp6a2lpsZtvvtlyc3MtNzfX6urq7LnnnrPc3FwrLy+3np4ea2tr6zOuubk5WEWTn59vI0eO7PMFALj0Deif4O68884LdoX+6le/alOnTrW//du/tQkTJtjQoUNt/fr1tnDhQjMz27Nnjx04cMBqamoGdGHvv/9+yrLDYcOGyXEqXlRUJMceOHAgGDt27FgwVl1dLedVZY633367HLtp06ZgrLCwMBjzSqlVKWkmuwyrHcePHz8uxw4fPjwY8/6Pyfvvvx+MqXJdb9fenTt3yriiHletUyZlsd6OyqqE/h/+4R/SnleVnHvl0up++/DDD9MaZ2a2a9euYMz7XaDKsNXz8XYjV7yxKq7uGa/kXMW9ezFVq0x/xp03oN80RUVFF/SljBgxwkaPHt37/YceesiWLVtmpaWlNnLkSFuyZInV1NTYrbfeOpCHAgBc4i76eUDPPPOMDRkyxBYuXNinERUAgE/KOAG99dZbff67oKDAVq1aZatWrcp0agDAJYy94AAAUZCAAABRkIAAAFGQgAAAUVz0KriLZfPmzSm39Pa2wFfboYdq1s9TvQ5qXm8bdVW/v379+rTHqr6Z8ePHy3lVf4XXj6B6rVS/lDrSwkwfpeFt6a+OvFC9PN4xA4cOHQrGxo4dK8eq4z3U0RRez00mx0s8/PDDwZjqB1H3mhf3ekLUe+vll18Oxrzn2t7eHox5O7mo/rARI0YEY959ql5b1Rtmpp9PVVVVMOb1BKrXTh3bYha+5v72AfEJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEEXWlmH39PSkLLPcvn27HKdKhL3SS1XiXVlZGYx5pZfd3d3BmNr23ZtbXZNXepnuvGZmR44cCcZUWXNLS4ucN5NSXhXPpLxbXZN3lEO6Ry547QIqfvDgQTnWOxohxCvHVWvhrbGaW62Tel+Z6eNKvNYJtU7q6AnvtVMl9N79lO77wzvKRL0vR48eLcd2dHTIuIdPQACAKEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgChIQACAKLK2D2jy5Mkpa/H3798vx6k+IK/nRtX3jxkzJhjzeivUFuxdXV1yrKrhV70MXu+FuuajR4/Ksar/4vjx48GYd8yD4h0H4PV1hHj9Req5en1lKq6ej3efqmv27sV9+/YFY+p4Ca8fJC8vLxj7zW9+I8eq97RaQ+/4AtUH5PWvqH429f5QvyfM9JEXXl+T6iFS7y3vHlfHS6jHNDMrLS1N+f3+vh/5BAQAiIIEBACIggQEAIiCBAQAiIIEBACIggQEAIgia8uwhw8fnrIEMNURDZ+kShm9MmBVjtja2hqMTZo0Sc578uTJYGzixIly7Pjx44OxUAmkmS6HNtPl6qoc3cysuLg4GPPKUJVMjmNQr11bW1swpsptzfQ6qfJ6M7MJEyYEY83NzcHYuHHj5Lxq2351r5mZbdy4MRhTpdZeGbxa/927d8ux6r2lStJV6bdZZuXqqoy4qKgoGFNHlZiZlZWVBWPeGqv7LZMWB/U70zsiIlRWrsrNP4lPQACAKEhAAIAoSEAAgChIQACAKEhAAIAoSEAAgCiytgz77NmzKUuu0y0LNPN3dlUllKpct6SkJO15vbJNVYatSq1PnDgh51UlrJmUuh88eDAY+73f+z05ryp99UpUVem4mtdbp/b29mBMlWh7j6vW39sNW62FV1audp7+7//+72DMK+tXpePePa5K99Vu8JnsGq5Kqc10abjaNdwrg1e80mW1+7ca67UL5OfnB2Pp7tDd37JwPgEBAKIgAQEAoiABAQCiIAEBAKIgAQEAoiABAQCiIAEBAKLI2j6gJEksSZILvu/1qHjb9iunTp0KxlSfiVfzrursVcy7JtVLUlBQIOdVfRBdXV1yrNq2X/UceH1YqofC69dR94WKeb0Xqg/C601S94V6Pt49oXrhDh06JMeqa1ZHFKgjLcx0z5PXm6ReH/V+9u5x1SeU6nfLJ6n3lnpdvWMe1HtLHWlhpq9Zrb93TV4/lRJ6v3u9R+fxCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABBF1pZhnzt3LmW5qVc2m245rpne7lyVvnql36q8uLq6Wo5N93gJr7RSlW12dHTIsZWVlcGYWif1mGa6zFRty2+my4TV66pKyr15VWm+Wfrl94WFhXJedUSEeq5m6R/D4c2rePeier7qvaXWwUyXaXvvWfXeUu/n/pYfp+K9P9L9XeC1Vagyba8lIHTPeKXf5/EJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQRdb2AX300Ucp+0K8+n21Vbq3fXu6Nfxe/X5ra2sw5m1VX1xcHIz1t9Y+FdU3oI4gMDNrbm4OxkaNGpX2vKoPxeslUWuhjj7wXvOSkpJgTPU8melrVv04an3NdG+Gd483NDQEY+Xl5XKsoh7Xe+0G6+gD9dp6R6io94d6rl5vmLpm74iIdI+m8I4NUdfk9U6Geri89/p5fAICAERBAgIAREECAgBEQQICAERBAgIARJF1VXDnK0FCFUZepYiKeztpq3gmuzyrsV4VlqomUdUr3jWpeb1rUnOreTN57TzqmlXMq9ZRz9WrglNj1XP11l9VcHn3uKqkyuSeyGQXaPW4ag1j7TytKsO8eTN57dK9j71ryqQKLvS457/vvadzkkze9YPg4MGDNmHChNiXAQDIUENDg1VVVQXjWZeAzp07Z42NjVZUVGQ5OTnW0dFhEyZMsIaGBvdMmMsZ69Q/rFP/sE79wzqlliSJdXZ2WmVlpfzUl3X/BDdkyJCUGXPkyJG8wP3AOvUP69Q/rFP/sE4XUk3051GEAACIggQEAIgi6xNQfn6+PfXUU+7Z5Jc71ql/WKf+YZ36h3XKTNYVIQAALg9Z/wkIAHBpIgEBAKIgAQEAoiABAQCiyPoEtGrVKrvyyiutoKDAZs+ebe+8807sS4rq7bfftnvvvdcqKystJyfHXn755T7xJEnsySeftHHjxtmwYcNs3rx5tnfv3jgXG8mKFSvslltusaKiIhs7dqzdd999tmfPnj4/c/r0aVu0aJGNHj3aCgsLbeHChe5JpJea1atX24033tjbRFlTU2P/8z//0xtnjVJbuXKl5eTk2GOPPdb7PdYqPVmdgP7rv/7Lli1bZk899ZRt3brVpk2bZvPnz7eWlpbYlxZNV1eXTZs2zVatWpUy/s1vftOee+45+973vmebNm2yESNG2Pz5891jkS8ldXV1tmjRItu4caO9/vrrdubMGbvrrrusq6ur92eWLl1qr776qq1du9bq6uqssbHR7r///ohX/emrqqqylStXWn19vW3ZssXmzp1rCxYssPfee8/MWKNUNm/ebN///vftxhtv7PN91ipNSRabNWtWsmjRot7/Pnv2bFJZWZmsWLEi4lVlDzNL1q1b1/vf586dSyoqKpKnn36693ttbW1Jfn5+8sMf/jDCFWaHlpaWxMySurq6JEk+XpOhQ4cma9eu7f2ZXbt2JWaWbNiwIdZlZoVRo0Yl//qv/8oapdDZ2ZlMmTIlef3115Pbb789efTRR5Mk4X7KRNZ+Aurp6bH6+nqbN29e7/eGDBli8+bNsw0bNkS8suy1b98+a2pq6rNmxcXFNnv27Mt6zdrb283MrLS01MzM6uvr7cyZM33WaerUqVZdXX3ZrtPZs2ettrbWurq6rKamhjVKYdGiRXbPPff0WRMz7qdMZN1mpOcdPXrUzp49a+Xl5X2+X15ebrt37450VdmtqanJzCzlmp2PXW7OnTtnjz32mN122212/fXXm9nH65SXl2clJSV9fvZyXKcdO3ZYTU2NnT592goLC23dunV27bXX2rZt21ijT6itrbWtW7fa5s2bL4hxP6UvaxMQcDEsWrTIdu7caf/3f/8X+1Ky0tVXX23btm2z9vZ2+9GPfmQPPvig1dXVxb6srNLQ0GCPPvqovf7661ZQUBD7ci4pWftPcGVlZXbFFVdcUEnS3NxsFRUVka4qu51fF9bsY4sXL7af/OQn9uabb/Y54qOiosJ6enqsra2tz89fjuuUl5dnkydPthkzZtiKFSts2rRp9u1vf5s1+oT6+npraWmxm2++2XJzcy03N9fq6ursueees9zcXCsvL2et0pS1CSgvL89mzJhh69ev7/3euXPnbP369VZTUxPxyrLXpEmTrKKios+adXR02KZNmy6rNUuSxBYvXmzr1q2zN954wyZNmtQnPmPGDBs6dGifddqzZ48dOHDgslqnVM6dO2fd3d2s0SfceeedtmPHDtu2bVvv18yZM+0rX/lK7/9mrdIUuwpCqa2tTfLz85MXX3wx+fWvf5187WtfS0pKSpKmpqbYlxZNZ2dn8u677ybvvvtuYmbJt771reTdd99N9u/fnyRJkqxcuTIpKSlJXnnllWT79u3JggULkkmTJiWnTp2KfOWfnkceeSQpLi5O3nrrreTw4cO9XydPnuz9mYcffjiprq5O3njjjWTLli1JTU1NUlNTE/GqP31PPPFEUldXl+zbty/Zvn178sQTTyQ5OTnJz3/+8yRJWCPlk1VwScJapSurE1CSJMl3vvOdpLq6OsnLy0tmzZqVbNy4MfYlRfXmm28mZnbB14MPPpgkycel2F//+teT8vLyJD8/P7nzzjuTPXv2xL3oT1mq9TGzZM2aNb0/c+rUqeQv//Ivk1GjRiXDhw9P/uiP/ig5fPhwvIuO4M///M+TiRMnJnl5ecmYMWOSO++8szf5JAlrpPxuAmKt0sNxDACAKLL2b0AAgEsbCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQBQkIABAFCQgAEAUJCAAQxf8DcRXQizaBRfcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_l[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBSQqXYIMNyP",
        "outputId": "c79e4b88-29a4-4618-82c9-b638b223172e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 5) # number of input channel is 1, number of output is 32, 5 is kernel size (the filter window in convolution)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 5) # the output of above here is input to this one they should match\n",
        "    self.conv3 = nn.Conv2d(64, 128, 5) # at this point you need to have one to two linear CN , these convolutions could come one after another in NN\n",
        "\n",
        "    x = torch.rand(50,50).view(-1,1,50,50)\n",
        "    self._to_linear = None\n",
        "    self.convs(x) #here we are passing randon datapoint to the forward pass to figure out the input numbers required for linear function, see the explaination below\n",
        "\n",
        "    # self.x = nn.Linear(128*2*2, 512) # this came by running the above and printing (x[0].shape) which comes as [128, 2, 2] search for I'm here\n",
        "    # self.y = nn.Linear(512, 2)\n",
        "\n",
        "    self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
        "    self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
        "\n",
        "  def convs(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "    x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
        "\n",
        "    # print(x[0].shape) #I'm here\n",
        "\n",
        "    if self._to_linear is None:\n",
        "      self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2] #this is dimention of the linear output we want\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convs(x) # here all the convolutions we pass, basically forward pass\n",
        "    x= x.view(-1, self._to_linear) # then we flatten it with view\n",
        "    x = F.relu(self.fc1(x)) # again fully connected then reLU\n",
        "    x= self.fc2(x)   # again fully connected\n",
        "    return F.softmax(x, dim=1) #dim is for batches, dim=0 → across batches (4 samples) # dim=1 → across classes (3 categories per sample)\n",
        "\n",
        "net = Net()\n",
        "print(net)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTCCrv5F3k93",
        "outputId": "93b4cc2a-d294-4410-dbb5-fe0841b0fe41"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What are Channels?**\n",
        "\n",
        "**Channels** represent different \"layers\" or \"features\" of your data:\n",
        "\n",
        "1. **Input Channels**:\n",
        "   - For grayscale images: 1 channel (just intensity values)\n",
        "   - For RGB color images: 3 channels (Red, Green, Blue)\n",
        "   - For other data: could be different feature maps\n",
        "\n",
        "2. **Output Channels**:\n",
        "   - These are the number of different feature detectors/filters the convolutional layer learns\n",
        "   - Each output channel learns to detect different patterns (edges, textures, shapes, etc.)\n",
        "\n",
        "In your code:\n",
        "```python\n",
        "self.conv1 = nn.Conv2d(1, 32, 5)    # 1 input channel → 32 output channels\n",
        "self.conv2 = nn.Conv2d(32, 64, 5)   # 32 input channels → 64 output channels  \n",
        "self.conv3 = nn.Conv2d(64, 128, 5)  # 64 input channels → 128 output channels\n",
        "```\n",
        "\n",
        "## **Why Do You Need Linear Layers?**\n",
        "\n",
        "**Linear layers** (fully connected layers) are needed because:\n",
        "\n",
        "1. **Convolutional layers** extract spatial features but maintain spatial structure\n",
        "2. **Linear layers** make the final classification decision by combining all features\n",
        "3. You need to **flatten** the 2D feature maps into a 1D vector before feeding to linear layers\n",
        "\n",
        "## **How Input/Output Dimensions Are Determined**\n",
        "\n",
        "Your clever approach calculates dimensions dynamically:\n",
        "\n",
        "```python\n",
        "x = torch.rand(50,50).view(-1,1,50,50)  # Create dummy input: [batch, channels, height, width]\n",
        "self._to_linear = None\n",
        "self.convs(x)  # Pass through conv layers to see output size\n",
        "```\n",
        "\n",
        "**The dimension calculation process:**\n",
        "\n",
        "1. **Input**: `[batch, 1, 50, 50]` (grayscale 50×50 images)\n",
        "\n",
        "2. **After conv1 + maxpool**: `[batch, 32, 23, 23]`\n",
        "   - Conv reduces: 50 - 5 + 1 = 46\n",
        "   - Maxpool reduces: 46 / 2 = 23\n",
        "\n",
        "3. **After conv2 + maxpool**: `[batch, 64, 9, 9]`\n",
        "   - Conv reduces: 23 - 5 + 1 = 19  \n",
        "   - Maxpool reduces: 19 / 2 = 9 (rounded down)\n",
        "\n",
        "4. **After conv3 + maxpool**: `[batch, 128, 2, 2]`\n",
        "   - Conv reduces: 9 - 5 + 1 = 5\n",
        "   - Maxpool reduces: 5 / 2 = 2 (rounded down)\n",
        "\n",
        "5. **Flattening**: 128 × 2 × 2 = 512 features\n",
        "\n",
        "That's why you have:\n",
        "```python\n",
        "self.x = nn.Linear(128*2*2, 512)  # 512 input features → 512 hidden units\n",
        "self.y = nn.Linear(512, 2)        # 512 hidden units → 2 output classes\n",
        "```\n"
      ],
      "metadata": {
        "id": "FsOdhAkaDAPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "loss_function = nn.MSELoss() #Mean Squared Error\n",
        "\n",
        "training_data_i = np.load(\"training_images.npy\", allow_pickle=True)\n",
        "training_data_l = np.load(\"training_labels.npy\", allow_pickle=True)\n",
        "\n",
        "X = torch.Tensor(training_data_i).view(-1, 50, 50) #flatten array\n",
        "X= X/255.0 # scaling the imagary, as input is between o to 255 here we are makaing it between 0 to 255\n",
        "y= torch.Tensor(training_data_l)\n",
        "\n",
        "VAL_PCT = 0.1 # training, testing, validation data separation. valaidation percentage\n",
        "Val_size = int(len(X)*VAL_PCT)\n",
        "print(Val_size)\n"
      ],
      "metadata": {
        "id": "N4NybhBOGO87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d59c8f-749d-4437-a563-2bb856fa1429"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = X[:-Val_size]\n",
        "train_y = y[:-Val_size]\n",
        "\n",
        "test_X = X[-Val_size:]\n",
        "test_y = y[-Val_size:]\n",
        "\n",
        "print(f\"train size: {len(train_X)}\")\n",
        "print(f\"test size: {len(test_X)}\")"
      ],
      "metadata": {
        "id": "Lir97-Ak--Vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e83e498-f60e-4a8e-9a7d-64a61d5820ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 22520\n",
            "test size: 2502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100 #If you are hitting memory you can definitely try batch size, if the batch size less than 8 doesn't work the start twiking model itself\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
        "    #print(i)\n",
        "    #print(i+BATCH_SIZE)\n",
        "    batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50) #view flattens\n",
        "    batch_y = train_y[i:i+BATCH_SIZE]\n",
        "\n",
        "    net.zero_grad #reset the gradients of the model's parameters to zero. This is a crucial step before calculating gradients for the current batch of data during the training process.\n",
        "    outputs = net(batch_X)\n",
        "    loss = loss_function(outputs, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"Epoch: {epoch}. Loss: {loss}\") # If loss decreasing per epoch then we are learning something\n",
        "\n",
        "print(loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "TXRPvMZbnxr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63c7ec0-919f-4b3c-8a8b-4ba5761d6c08"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 226/226 [01:42<00:00,  2.21it/s]\n",
            "100%|██████████| 226/226 [01:49<00:00,  2.05it/s]\n",
            "100%|██████████| 226/226 [01:41<00:00,  2.22it/s]\n",
            "100%|██████████| 226/226 [01:40<00:00,  2.24it/s]\n",
            "100%|██████████| 226/226 [01:48<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5230, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # you can use eval and train from model as well\n",
        "  for i in tqdm(range(len(test_X))):\n",
        "    real_class = torch.argmax(test_y[i])\n",
        "    net_out = net(test_X[i].view(-1, 1, 50, 50))[0]\n",
        "    predicted_class = torch.argmax(net_out)\n",
        "\n",
        "    if predicted_class == real_class:\n",
        "      correct += 1\n",
        "    total += 1\n",
        "print(correct)\n",
        "print(\"Accuracy: \", round(correct/total, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBuHWO0Wa3xF",
        "outputId": "651f5641-1e02-4339-9bd4-c74ac4f22001"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2502/2502 [00:05<00:00, 452.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1270\n",
            "Accuracy:  0.508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the accuracy is 50.8% how can I improve further\n",
        "\n",
        "\n",
        "Improving model accuracy can involve several strategies. Here's a plan to explore some common techniques:\n",
        "\n",
        "* Increase epochs: Train the model for more epochs to allow it more time to learn from the data. (I changed from 1 to 5 but it did not affect accuracy)\n",
        "* Adjust learning rate: Experiment with a different learning rate for the optimizer. A smaller learning rate might lead to better convergence, while a larger one could speed up training but risk overshooting the minimum.(tried with 0.001, 0.01 then now 0.0001 not changed  0.508)\n",
        "* Modify model architecture:\n",
        "Add more layers (convolutional or linear) to potentially capture more complex features.\n",
        "Change the kernel size or number of filters in the convolutional layers.\n",
        "Add dropout layers to reduce overfitting.\n",
        "* Change loss function: While MSELoss can work for classification, using a loss function more suited for classification like Cross-Entropy Loss is generally recommended.\n",
        "* Data augmentation: Implement data augmentation techniques (e.g., random flips, rotations, zooms) to increase the size and variability of the training data, which can improve generalization.\n",
        "* Batch size tuning: Experiment with different batch sizes.\n",
        "* Check data preprocessing: Revisit the data loading and preprocessing steps to ensure they are optimal.\n",
        "* Regularization: Add L1 or L2 regularization to the model to prevent overfitting.\n",
        "* Advanced optimization: Consider using more advanced optimizers like AdamW or RMSprop.\n",
        "* Transfer learning: For image tasks, using a pre-trained model on a large dataset like ImageNet and fine-tuning it on your specific task can significantly boost performance.\n",
        "* Finish task: Summarize the improvements made and the final accuracy achieved.\n",
        "\n"
      ],
      "metadata": {
        "id": "0cdB1BdzduCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "what is Regularization:\n",
        "\n",
        "\n",
        "Add L1 or L2 regularization to the model to prevent overfitting.\n",
        "L1 and L2 regularization are techniques used in machine learning to prevent overfitting. Here's a breakdown:\n",
        "\n",
        "* Overfitting: This happens when a model learns the training data too well, including the noise and outliers. As a result, the model performs poorly on new, unseen data.\n",
        "* Regularization: These techniques add a penalty to the loss function based on the magnitude of the model's weights. This encourages the model to use smaller weights, which generally leads to simpler models that are less prone to overfitting.\n",
        "Here's how L1 and L2 regularization differ:\n",
        "\n",
        "* L1 Regularization (Lasso Regression): Adds a penalty proportional to the absolute value of the weights ($\\sum |w|$$\\sum |w|$). This can lead to some weights becoming exactly zero, effectively performing feature selection.\n",
        "* L2 Regularization (Ridge Regression): Adds a penalty proportional to the square of the weights ($\\sum w^2$$\\sum w^2$). This encourages weights to be small but rarely exactly zero. It tends to shrink all weights proportionally.\n",
        "In the context of your neural network, adding L1 or L2 regularization would involve modifying the loss function to include a term that penalizes large weights in the convolutional and linear layers. This would help prevent the network from becoming too complex and overfitting to the training data."
      ],
      "metadata": {
        "id": "DtzMeHVZfUz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.eye(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeSMEJ20wHrt",
        "outputId": "abb393aa-76e6-4325-94e3-992d5a0e3de3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.eye(10)[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLN0e7V91_-y",
        "outputId": "ace9dab7-c051-43d9-9cec-f7254d89f41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}
