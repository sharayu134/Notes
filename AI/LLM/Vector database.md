* mathematical representation of structured and unstructured data
* these representation of data is easily understandable by machine learning models
* when we query vector database we will get embeddings of relavent data
* this data is fed to LLM which will provide output


### **ğŸ” How Does Vector Search Work?**  

Vector search is a method used to **find similar items** based on their vector representations. Instead of relying on traditional keyword matching (like in relational databases), vector search finds **semantic similarities** between data points.  

---

## **ğŸ§© Step-by-Step Process of Vector Search**
### **1ï¸âƒ£ Convert Data into Vectors (Embeddings)**
- Raw data (e.g., text, images, audio, user behavior) is **converted into numerical vectors** using an embedding model.
- Examples of embedding models:
  - **Text**: OpenAI, BERT, Sentence Transformers  
  - **Images**: CLIP, ResNet  
  - **Audio**: Wav2Vec, DeepSpeech  

#### **Example**
ğŸ“œ **Text Input:** `"Paris is a beautiful city"`  
â¡ **Embedding Output (Vector Representation):**  
```
[0.12, -0.34, 0.87, ..., 0.05]  (768 dimensions)
```

---

### **2ï¸âƒ£ Store Vectors in a Vector Database**
- The generated vectors are stored in a **vector database** (e.g., Qdrant, Pinecone, Weaviate, FAISS).
- Each vector is associated with **metadata** (e.g., document ID, image URL, product details).

---

### **3ï¸âƒ£ Perform a Vector Search**
- When a user provides a **query** (e.g., "Best places in France"), it is **converted into a vector**.
- The database **compares this vector** to stored vectors using a similarity metric.

---

### **4ï¸âƒ£ Compute Similarity Using Distance Metrics**
- **Similarity is measured using distance functions:**
  1. **Cosine Similarity** (measures angle between vectors)  
  2. **Euclidean Distance** (measures straight-line distance)  
  3. **Dot Product** (measures magnitude and direction)

#### **Example: Cosine Similarity Formula**
\[
\text{similarity} = \frac{A \cdot B}{||A|| \times ||B||}
\]
Where:
- \(A\) = Query vector  
- \(B\) = Stored vector  

If similarity **â‰ˆ 1**, the vectors are **very similar**.  

---

### **5ï¸âƒ£ Return the Most Relevant Results**
- The **top N results** with the highest similarity scores are returned.
- These results are typically sorted in **descending order of similarity**.

#### **Example Search Results**
| Rank | Text / Image | Cosine Similarity |
|------|-------------|------------------|
| 1ï¸âƒ£  | `"Eiffel Tower is a must-visit"` | **0.95** |
| 2ï¸âƒ£  | `"Visit France for amazing architecture"` | **0.90** |
| 3ï¸âƒ£  | `"Top tourist attractions in Paris"` | **0.85** |

---

## **ğŸ›  Example: Running a Vector Search in Qdrant**
If you have text embeddings stored, you can perform a search like this:

### **ğŸ”¹ Query Example**
```json
{
  "vector": { "text_embedding": [0.12, -0.34, 0.87, ..., 0.05] },
  "top": 5
}
```
â¡ This will return **top 5 most relevant results** based on similarity.

---

## **ğŸš€ Why Use Vector Search?**
âœ… **Finds semantic matches** â€“ Works better than keyword search  
âœ… **Handles multi-modal data** â€“ Supports text, images, audio, and more  
âœ… **Fast & Scalable** â€“ Optimized for large datasets with millions of vectors  
âœ… **Improves recommendations** â€“ Used in search engines, chatbots, and personalization  

To see the detailed query run 
```
docker pull qdrant/qdrant
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant
```
### **ğŸ”¢ What is the "score" in the Vector Database Result?**  

The **"score"** in your Qdrant vector search result represents the **similarity or relevance** between the **query vector** and the stored vectors.  

---

### **ğŸ” Understanding the Score**
- **Higher score** â†’ **More similar** to the query  
- **Lower score** â†’ **Less similar** to the query  

The meaning of the score depends on the **distance metric** used in the vector search.  

---

### **ğŸ“ Common Distance Metrics & Their Scores**
| Distance Metric        | Score Meaning |
|------------------------|--------------|
| **Cosine Similarity**  | **Higher = Better match** (Ranges from -1 to 1) |
| **Dot Product**        | **Higher = Better match** (No fixed range) |
| **Euclidean Distance** | **Lower = Better match** (Ranges from 0 to âˆ) |

#### **Example: Cosine Similarity**
\[
\text{similarity} = \frac{A \cdot B}{||A|| \times ||B||}
\]
- If **score = 1** â†’ Perfect match  
- If **score = 0.03** (like "Neptune") â†’ Weak match  

---

### **ğŸš€ Interpreting Your Data**
Based on your JSON:
```json
"score": 0.031177998,  // Neptune
"score": 0.022683999,  // Mars
"score": 0.0123890005, // Venus
"score": 0.008311      // Jupiter
```
- **Neptune (0.0311)** is the closest match to the query.  
- **Jupiter (0.0083)** is the least similar match.  
### **ğŸ”¢ How to Compute the Dot Product of Two Vectors?**  

The **dot product** (also called the **inner product**) of two vectors measures their similarity and is calculated as:

\[
A \cdot B = A_1B_1 + A_2B_2 + A_3B_3 + \dots + A_nB_n
\]

Where:  
- **\(A\)** and **\(B\)** are two vectors  
- **\(A_i\)** and **\(B_i\)** are the corresponding elements of each vector  

---

### **ğŸ›  Methods to Compute Dot Product**
#### **1ï¸âƒ£ Python (NumPy)**
```python
import numpy as np

A = np.array([1, 2, 3])
B = np.array([4, 5, 6])

dot_product = np.dot(A, B)  # (1*4) + (2*5) + (3*6) = 32
print(dot_product)  # Output: 32
```

#### **2ï¸âƒ£ Python (Without NumPy)**
```python
A = [1, 2, 3]
B = [4, 5, 6]

dot_product = sum(a * b for a, b in zip(A, B))
print(dot_product)  # Output: 32
```

#### **3ï¸âƒ£ Using Matrix Multiplication (NumPy)**
```python
dot_product = A @ B  # Also works in NumPy
print(dot_product)  # Output: 32
```

---

### **ğŸ” Example Calculation**
If we have:
\[
A = [1, 2, 3], \quad B = [4, 5, 6]
\]
The dot product is:
\[
(1 \times 4) + (2 \times 5) + (3 \times 6) = 4 + 10 + 18 = 32
\]

---

### **ğŸ“Œ Applications of Dot Product**
- **Vector similarity** in **vector databases** (e.g., Qdrant, Pinecone, FAISS)  
- **Machine learning** (e.g., embeddings, recommendation systems)  
- **Physics & engineering** (e.g., projections, work done by force)  

Would you like help using this in a vector search query? ğŸš€
---

### **ğŸ›  How to Improve Results?**
- **Use a different similarity metric** (e.g., switch from Cosine to Dot Product).  
- **Increase embedding quality** (try a better embedding model).  
- **Adjust query vector** (modify weights to emphasize certain features).  
