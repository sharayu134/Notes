* LLMs are instances of foundation **models**
* foundation models are pre-trained on unlabelled and self-supervised data
* models learns from patterns in data in generalisable and adaptable way
* this data is text, text, and text like things (code, books, articles, conversations ) **language**
* this data is enourmous in petabytes
* 1GB of text file has 178million words -> 1PB has 1million GB this is **large**
* even the parameters can be in billions
* LLM - data + architecture + training
* Architecture -> transformer -> handles sequences of data, understand context of each word in relation to every other word
* Training -> predict next words in a sentense
* like to predict whats next to **the sky is blue..** it will predict something like **bug** and after itrations it will figure out it's **blue**
* finetuning can make general model expert in specific task
